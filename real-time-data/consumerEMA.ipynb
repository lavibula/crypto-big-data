{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.chdir(\"/Users/ibulmnie/Documents/20241/BigData/crypto-big-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhh\n"
     ]
    }
   ],
   "source": [
    "print(\"hhhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKafkaConnectionTest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m35.206.252.44:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest-topic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\sql\\streaming\\readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConnectionTest\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import col, from_json, to_date, avg, lit, to_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "gcs_jar_path = os.path.abspath(r\"D:\\Empty\\crypto-big-data\\config\\gcs-connector-hadoop3-latest.jar\")\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"hehee\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", r\"D:\\Empty\\crypto-big-data\\config\\btcanalysishust-d7c3a4830bef.json\") \\\n",
    "    .config(\"spark.jars\", gcs_jar_path) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-core_2.12:2.4.0\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m      2\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[0;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprices\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType([\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     ]))\n\u001b[0;32m     23\u001b[0m ])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Đọc dữ liệu từ Kafka\u001b[39;00m\n\u001b[0;32m     26\u001b[0m kafka_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m35.206.252.44:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrypto-pricessss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxOffsetsPerTrigger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m parsed_df \u001b[38;5;241m=\u001b[39m kafka_df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mfrom_json(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m), schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     38\u001b[0m crypto_parsed_df \u001b[38;5;241m=\u001b[39m parsed_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     39\u001b[0m     F\u001b[38;5;241m.\u001b[39mto_timestamp(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHH:mm:ss.SSSSSSXXX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     40\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.prices.bitcoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m )\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\sql\\streaming\\readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "# Schema của dữ liệu Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-pricessss\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000)\\\n",
    "    .load()\n",
    "\n",
    "\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(F.col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "crypto_parsed_df = parsed_df.select(\n",
    "    F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "    F.col(\"data.prices.bitcoin\").alias(\"CLOSE\")\n",
    ")\n",
    "\n",
    "    # F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "    # F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "    # F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "    # F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "    # F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "    # F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "    # F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "    # F.col(\"data.prices.solana\").alias(\"SOL\"),\n",
    "    # F.col(\"data.prices.litecoin\").alias(\"LTC\"),\n",
    "    # F.col(\"data.prices.polkadot\").alias(\"DOT\"),\n",
    "    # F.col(\"data.prices.shiba-inu\").alias(\"SHIB\"),\n",
    "    # F.col(\"data.prices.tron\").alias(\"TRX\"),\n",
    "    # F.col(\"data.prices.cosmos\").alias(\"ATOM\"),\n",
    "    # F.col(\"data.prices.chainlink\").alias(\"LINK\"),\n",
    "    # F.col(\"data.prices.stellar\").alias(\"XLM\"),\n",
    "    # F.col(\"data.prices.near\").alias(\"NEAR\")\n",
    "# )\n",
    "\n",
    "# # Định nghĩa schema cho dữ liệu lịch sử\n",
    "historical_schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),#là mã của crypto, ví dụ BASE BTC sẽ có các row chứa dữ liệu theo các cột kế bên\n",
    "    StructField(\"DATE\", TimestampType(), True),\n",
    "    StructField(\"OPEN\", FloatType(), True),\n",
    "    StructField(\"HIGH\", FloatType(), True),\n",
    "    StructField(\"LOW\", FloatType(), True),\n",
    "    StructField(\"CLOSE\", FloatType(), True),\n",
    "    StructField(\"VOLUME\", FloatType(), True),\n",
    "    StructField(\"YEAR\", StringType(), True),\n",
    "    StructField(\"MONTH\", StringType(), True),\n",
    "    StructField(\"__index_level_0__\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:40:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/k9/xss4jl9s24sg7_7zhcmrp2g00000gn/T/temporary-c0ebada1-cde1-4576-8139-dd3b88b955ad. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/12/13 22:40:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/12/13 22:40:45 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/12/13 22:40:46 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/12/13 22:40:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:40:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE               |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+-------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 00:00:00|100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00|100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00|101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00|96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00|97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00|101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00|99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00|99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00|97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00|98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00|95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00|95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00|96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00|97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00|95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00|95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00|91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00|93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00|98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "|2024-11-23 00:00:00|97778.77|96414.18600000002|93151.292        |86063.96849999999|74350.98619999998|67406.98700000002|65715.11214999999|\n",
      "+-------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:41:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:41:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:41:04.301352|101312.0|99856.48000000001|99471.23999999999|97688.5365       |87386.08719999997|74944.6014       |68894.94369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:42:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:42:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:42:04.999568|101312.0|99856.48000000001|99471.23999999999|97688.5365       |87386.08719999997|74944.6014       |68894.94369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:43:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:43:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:43:05.188914|101180.0|99830.08         |99458.04         |97681.9365       |87383.44719999997|74943.28139999999|68894.28369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:44:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:44:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:44:03.891276|101180.0|99830.08         |99458.04         |97681.9365       |87383.44719999997|74943.28139999999|68894.28369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:45:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:45:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:45:04.724595|101180.0|99830.08         |99458.04         |97681.9365       |87383.44719999997|74943.28139999999|68894.28369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:46:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:46:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:46:03.989936|101180.0|99830.08         |99458.04         |97681.9365       |87383.44719999997|74943.28139999999|68894.28369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:47:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:47:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:47:04.356353|101180.0|99830.08         |99458.04         |97681.9365       |87383.44719999997|74943.28139999999|68894.28369999999|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:48:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:48:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                     |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+-------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:48:04.29577|101180.0|99830.08         |99458.04         |97681.9365       |87383.44719999997|74943.28139999999|68894.28369999999|\n",
      "|2024-12-13 00:00:00      |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00      |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00      |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00      |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00      |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00      |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00      |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00      |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00      |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00      |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00      |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00      |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00      |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00      |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00      |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00      |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00      |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00      |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00      |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+-------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 22:49:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/13 22:49:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ibulmnie/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ibulmnie/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                      |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 22:49:05.188617|100631.0|99720.28         |99403.13999999998|97654.4865       |87372.46719999997|74937.7914       |68891.53869999998|\n",
      "|2024-12-13 00:00:00       |100088.2|99074.06         |99215.75         |97511.875        |86722.98399999997|74511.1914       |68735.22729999998|\n",
      "|2024-12-12 00:00:00       |100050.1|99290.42         |98798.68000000001|97458.66650000002|86053.24219999996|74084.9978       |68577.15489999998|\n",
      "|2024-12-11 00:00:00       |101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10 00:00:00       |96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09 00:00:00       |97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08 00:00:00       |101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07 00:00:00       |99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06 00:00:00       |99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05 00:00:00       |97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04 00:00:00       |98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03 00:00:00       |95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02 00:00:00       |95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30 00:00:00       |96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00       |97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28 00:00:00       |95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00       |95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00       |91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25 00:00:00       |93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00       |98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "+--------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# combined_df.write \\\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# .format(\"csv\") \\\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# .option(\"header\", \"true\") \\\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# .option(\"path\", \"gs://indicator-crypto/sma_results/BTC\") \\\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# .mode(\"overwrite\") \\\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# .save()\u001b[39;00m\n\u001b[1;32m     46\u001b[0m query \u001b[38;5;241m=\u001b[39m crypto_parsed_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch) \\\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 50\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Đọc dữ liệu batch từ Parquet\n",
    "historical_data_df = (\n",
    "    spark.read.format(\"parquet\")\n",
    "    .load(\"gs://crypto-historical-data-2/ver2/BTC/2024/*\")\n",
    "    .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    ")\n",
    "\n",
    "# 3. Trigger micro-batch và xử lý\n",
    "def process_batch(micro_batch_df, batch_id):\n",
    "    # Lọc ra giá trị gần nhất của micro-batch (dòng có timestamp mới nhất)\n",
    "    micro_batch_latest_df = (\n",
    "        micro_batch_df\n",
    "        .withColumn(\"row_num\", F.row_number().over(Window.orderBy(F.col(\"DATE\").desc())))\n",
    "        .filter(F.col(\"row_num\") == 1)  # Chỉ lấy dòng có timestamp mới nhất\n",
    "        .drop(\"row_num\")  # Loại bỏ cột row_num\n",
    "    )\n",
    "    # Kết hợp micro-batch với dữ liệu batch (historical_data_df)\n",
    "    combined_df = (\n",
    "        micro_batch_latest_df\n",
    "        .unionByName(historical_data_df)  # Union với dữ liệu batch\n",
    "    )\n",
    "    \n",
    "    # Tính toán các chỉ số SMA với các cửa sổ khác nhau\n",
    "    window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    combined_df = combined_df.withColumn(\"SMA_5\", F.avg(\"CLOSE\").over(window_spec.rowsBetween(-4, 0)))\n",
    "    combined_df = combined_df.withColumn(\"SMA_10\", F.avg(\"CLOSE\").over(window_spec.rowsBetween(-9, 0)))\n",
    "    combined_df = combined_df.withColumn(\"SMA_20\", F.avg(\"CLOSE\").over(window_spec.rowsBetween(-19, 0)))\n",
    "    combined_df = combined_df.withColumn(\"SMA_50\", F.avg(\"CLOSE\").over(window_spec.rowsBetween(-49, 0)))\n",
    "    combined_df = combined_df.withColumn(\"SMA_100\", F.avg(\"CLOSE\").over(window_spec.rowsBetween(-99, 0)))\n",
    "    combined_df = combined_df.withColumn(\"SMA_200\", F.avg(\"CLOSE\").over(window_spec.rowsBetween(-199, 0)))\n",
    "\n",
    "    # Sắp xếp lại theo DATE\n",
    "    combined_df = combined_df.orderBy(\"DATE\", ascending=False)\n",
    "\n",
    "    # Ghi ra kết quả\n",
    "    combined_df.write.format(\"console\").option(\"truncate\", False).save()\n",
    "    # combined_df.write \\\n",
    "    # .format(\"csv\") \\\n",
    "    # .option(\"header\", \"true\") \\\n",
    "    # .option(\"path\", \"gs://indicator-crypto/sma_results/BTC\") \\\n",
    "    # .mode(\"overwrite\") \\\n",
    "    # .save()\n",
    "\n",
    "\n",
    "query = crypto_parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(KEY_FILE)\n",
    "\n",
    "client = storage.Client(\n",
    "    credentials=credentials, \n",
    "    project='btcanalysishust'\n",
    ")\n",
    "\n",
    "\n",
    "bucket = client.get_bucket(\"crypto-historical-data-2\")\n",
    "blob = bucket.blob(\"ver2/ADA/2021/03/data.parquet\")\n",
    "print(blob.exists())  # Should return True if the file exists\n",
    "# bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o577.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\r\n\tat scala.collection.immutable.List.map(List.scala:246)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 112\u001b[0m\n\u001b[0;32m    108\u001b[0m     historical_data_df \u001b[38;5;241m=\u001b[39m historical_data_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_10\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_20\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_50\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_200\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m historical_data_df\n\u001b[1;32m--> 112\u001b[0m historical_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_coin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBTC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m historical_data_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[27], line 98\u001b[0m, in \u001b[0;36mprocess_coin\u001b[1;34m(coin)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_coin\u001b[39m(coin):\n\u001b[1;32m---> 98\u001b[0m     historical_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_historical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     window_spec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrowsBetween(Window\u001b[38;5;241m.\u001b[39munboundedPreceding, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    101\u001b[0m     historical_data_df \u001b[38;5;241m=\u001b[39m historical_data_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMA_5\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mavg(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mover(window_spec\u001b[38;5;241m.\u001b[39mrowsBetween(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[27], line 93\u001b[0m, in \u001b[0;36mread_historical_data\u001b[1;34m(coin)\u001b[0m\n\u001b[0;32m     88\u001b[0m years \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(year) \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2021\u001b[39m, current_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]  \n\u001b[0;32m     90\u001b[0m paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://crypto-historical-data-2/ver2/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years]\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     94\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m )\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\sql\\readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    542\u001b[0m )\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o577.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\r\n\tat scala.collection.immutable.List.map(List.scala:246)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Explicitly set Python and library paths\n",
    "python_path = sys.executable\n",
    "spark_home = r\"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\"  # Adjust to your Spark installation path\n",
    "hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "java_home = r\"D:\\Empty\\Java\"\n",
    "spark_temp = r\"D:\\Empty\\spark-temp\"\n",
    "\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "# bucket_name = 'ba_bucket3'\n",
    "gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "# Add Spark and Python paths\n",
    "sys.path.append(os.path.join(spark_home, 'python'))\n",
    "sys.path.append(os.path.join(spark_home, 'python', 'lib', 'py4j-0.10.9.7-src.zip'))\n",
    "\n",
    "# gcs_jar_path = os.path.abspath(\"config/gcs-connector-hadoop3-latest.jar\")\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "# python_path = sys.executable\n",
    "# gcs_connector_jar = r\"D:\\Empty\\crypto-big-data\\config\\gcs-connector-hadoop3-latest.jar\n",
    "# hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "# gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "# KEY_FILE = r\"D:\\Empty\\crypto-big-data\\config\\btcanalysishust-d7c3a4830bef.json\"\n",
    "# spark_temp = r\"D:\\Empty\\spark-temp\"\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"hehee\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "#     .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,io.delta:delta-core_2.12:2.2.0\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "#     .getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Connector Test\") \\\n",
    "    .config(\"spark.pyspark.python\", python_path) \\\n",
    "    .config(\"spark.pyspark.driver.python\", python_path) \\\n",
    "    .config(\"spark.driver.extraLibraryPath\", os.path.dirname(python_path)) \\\n",
    "    .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.local.dir\", spark_temp) \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "column_names = [\"BTC\", \"ETH\", \"USDT\", \"USDC\", \"XRP\", \"ADA\", \"DOGE\", \"MATIC\", \"SOL\"]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),  \n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"OPEN\", DoubleType(), True),\n",
    "    StructField(\"HIGH\", DoubleType(), True),\n",
    "    StructField(\"LOW\", DoubleType(), True),\n",
    "    StructField(\"CLOSE\", DoubleType(), True),\n",
    "    StructField(\"VOLUME\", DoubleType(), True),\n",
    "    StructField(\"YEAR\", IntegerType(), True),\n",
    "    StructField(\"MONTH\", IntegerType(), True),\n",
    "    StructField(\"__index_level_0__\", LongType(), True)\n",
    "])\n",
    "\n",
    "import datetime\n",
    "\n",
    "def read_historical_data(coin):\n",
    "    current_year = datetime.datetime.now().year  \n",
    "    years = [str(year) for year in range(2021, current_year + 1)]  \n",
    "\n",
    "    paths = [f\"gs://crypto-historical-data-2/ver2/{coin}/{year}/*\" for year in years]\n",
    "    \n",
    "    return (\n",
    "        spark.read.schema(schema).parquet(*paths)  \n",
    "        .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    "    )\n",
    "\n",
    "def process_coin(coin):\n",
    "    historical_data_df = read_historical_data(coin)\n",
    "    window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    \n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_5\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-4, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_10\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-9, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_20\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-19, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_50\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-49, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_100\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-99, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_200\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-199, 0)))\n",
    "\n",
    "    historical_data_df = historical_data_df.select(\"DATE\", \"CLOSE\", \"SMA_5\", \"SMA_10\", \"SMA_20\", \"SMA_50\", \"SMA_100\", \"SMA_200\").orderBy(\"DATE\", ascending=False)\n",
    "    return historical_data_df\n",
    "\n",
    "\n",
    "historical_data_df = process_coin(\"BTC\")\n",
    "historical_data_df.show()\n",
    "    # tmp_dir = f\"gs://indicator-crypto/sma_results/batch/{coin}\"\n",
    "\n",
    "    # historical_data_df.write \\\n",
    "    #     .format(\"csv\") \\\n",
    "    #     .option(\"header\", \"true\") \\\n",
    "    #     .option(\"path\", tmp_dir) \\\n",
    "    #     .mode(\"append\") \\\n",
    "    #     .save()\n",
    "\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = [executor.submit(process_coin, coin) for coin in column_names]\n",
    "#     for future in as_completed(futures):\n",
    "#         print(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o619.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\r\n\tat scala.collection.immutable.List.map(List.scala:246)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 96\u001b[0m\n\u001b[0;32m     82\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     83\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBASE\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),  \n\u001b[0;32m     84\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m ])\n\u001b[0;32m     95\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://crypto-historical-data-2/ver2/ADA/2021/03/data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 96\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# blob = bucket.blob(\"ver2/ADA/2021/03/data.parquet\")\u001b[39;00m\n\u001b[0;32m     98\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\sql\\readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    542\u001b[0m )\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o619.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\r\n\tat scala.collection.immutable.List.map(List.scala:246)\r\n\tat scala.collection.immutable.List.map(List.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Explicitly set Python and library paths\n",
    "python_path = sys.executable\n",
    "spark_home = r\"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\"  # Adjust to your Spark installation path\n",
    "hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "java_home = r\"D:\\Empty\\Java\"\n",
    "spark_temp = r\"D:\\Empty\\spark-temp\"\n",
    "\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "# bucket_name = 'ba_bucket3'\n",
    "gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "# Add Spark and Python paths\n",
    "sys.path.append(os.path.join(spark_home, 'python'))\n",
    "sys.path.append(os.path.join(spark_home, 'python', 'lib', 'py4j-0.10.9.7-src.zip'))\n",
    "\n",
    "# gcs_jar_path = os.path.abspath(\"config/gcs-connector-hadoop3-latest.jar\")\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "# python_path = sys.executable\n",
    "# gcs_connector_jar = r\"D:\\Empty\\crypto-big-data\\config\\gcs-connector-hadoop3-latest.jar\n",
    "# hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "# gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "# KEY_FILE = r\"D:\\Empty\\crypto-big-data\\config\\btcanalysishust-d7c3a4830bef.json\"\n",
    "# spark_temp = r\"D:\\Empty\\spark-temp\"\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"hehee\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "#     .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,io.delta:delta-core_2.12:2.2.0\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "#     .getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"GCS Connector Test\") \\\n",
    "#     .config(\"spark.pyspark.python\", python_path) \\\n",
    "#     .config(\"spark.pyspark.driver.python\", python_path) \\\n",
    "#     .config(\"spark.driver.extraLibraryPath\", os.path.dirname(python_path)) \\\n",
    "#     .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.local.dir\", spark_temp) \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Connector Test\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "column_names = [\"BTC\", \"ETH\", \"USDT\", \"USDC\", \"XRP\", \"ADA\", \"DOGE\", \"MATIC\", \"SOL\"]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),  \n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"OPEN\", DoubleType(), True),\n",
    "    StructField(\"HIGH\", DoubleType(), True),\n",
    "    StructField(\"LOW\", DoubleType(), True),\n",
    "    StructField(\"CLOSE\", DoubleType(), True),\n",
    "    StructField(\"VOLUME\", DoubleType(), True),\n",
    "    StructField(\"YEAR\", IntegerType(), True),\n",
    "    StructField(\"MONTH\", IntegerType(), True),\n",
    "    StructField(\"__index_level_0__\", LongType(), True)\n",
    "])\n",
    "\n",
    "path = \"gs://crypto-historical-data-2/ver2/ADA/2021/03/data.parquet\"\n",
    "df = spark.read.parquet(path)\n",
    "# blob = bucket.blob(\"ver2/ADA/2021/03/data.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: An error occurred while calling o640.parquet.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n",
      "\tat scala.collection.immutable.List.map(List.scala:246)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 29 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_30936\\4288638905.py\", line 51, in <module>\n",
      "    df = spark.read.parquet(path)\n",
      "  File \"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\sql\\readwriter.py\", line 544, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o640.parquet.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n",
      "\tat scala.collection.immutable.List.map(List.scala:246)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 29 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Paths (ensure these match your system)\n",
    "PYTHON_PATH = sys.executable\n",
    "SPARK_HOME = r\"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\"\n",
    "HADOOP_HOME = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "JAVA_HOME = r\"D:\\Empty\\Java\"\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "GCS_CONNECTOR_JAR = r\"D:\\Empty\\hadoop\\hadoop\\share\\hadoop\\tools\\lib\\gcs-connector-hadoop3-latest.jar\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "os.environ['PYSPARK_PYTHON'] = PYTHON_PATH\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYTHON_PATH\n",
    "\n",
    "# Create Spark session with extensive GCS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS_Connector_Debugging\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", GCS_CONNECTOR_JAR) \\\n",
    "    .config(\"spark.executor.extraClassPath\", GCS_CONNECTOR_JAR) \\\n",
    "    .config(\"spark.jars\", GCS_CONNECTOR_JAR) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your Parquet file\n",
    "path = \"gs://crypto-historical-data-2/ver2/ADA/2021/03/data.parquet\"\n",
    "\n",
    "# Import necessary Java classes directly\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Attempt to read the file\n",
    "try:\n",
    "    # Create Hadoop Configuration\n",
    "    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "    hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "    hadoop_conf.set(\"google.cloud.auth.service.account.enable\", \"true\")\n",
    "    hadoop_conf.set(\"google.cloud.auth.service.account.json.keyfile\", KEY_FILE)\n",
    "\n",
    "    # Try reading the file\n",
    "    df = spark.read.parquet(path)\n",
    "    print(\"DataFrame schema:\")\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Explicitly set Python and library paths\n",
    "python_path = sys.executable\n",
    "spark_home = r\"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\"  # Adjust to your Spark installation path\n",
    "hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "java_home = r\"D:\\Empty\\Java\"\n",
    "\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "bucket_name = 'ba_bucket3'\n",
    "gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "# Add Spark and Python paths\n",
    "sys.path.append(os.path.join(spark_home, 'python'))\n",
    "sys.path.append(os.path.join(spark_home, 'python', 'lib', 'py4j-0.10.9.7-src.zip'))\n",
    "\n",
    "# spark = get_spark_session()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "# from config import *\n",
    "\n",
    "# def get_spark_session():\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Connector Test\") \\\n",
    "    .config(\"spark.pyspark.python\", python_path) \\\n",
    "    .config(\"spark.pyspark.driver.python\", python_path) \\\n",
    "    .config(\"spark.driver.extraLibraryPath\", os.path.dirname(python_path)) \\\n",
    "    .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.local.dir\", \"D:/Empty/spark-temp\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "            \n",
    "    # return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+------+------+------+--------------+----+-----+-----------------+\n",
      "|BASE|      DATE|  OPEN|  HIGH|   LOW| CLOSE|        VOLUME|YEAR|MONTH|__index_level_0__|\n",
      "+----+----------+------+------+------+------+--------------+----+-----+-----------------+\n",
      "| ADA|2021-03-31|1.2135|1.2138|1.1526|1.1934| 4.601137893E7|2021|    3|             1335|\n",
      "| ADA|2021-03-30|1.2032|1.2365|1.1888|1.2135| 4.517666711E7|2021|    3|             1336|\n",
      "| ADA|2021-03-29|1.1903|1.2294|1.1777|1.2032|  4.34716022E7|2021|    3|             1337|\n",
      "| ADA|2021-03-28|1.1768|1.2188|1.1666|1.1903| 3.909916673E7|2021|    3|             1338|\n",
      "| ADA|2021-03-27|1.2144|1.2415|1.1575|1.1768| 4.792371762E7|2021|    3|             1339|\n",
      "| ADA|2021-03-26|1.0963|1.2869|1.0944|1.2144| 7.374575707E7|2021|    3|             1340|\n",
      "| ADA|2021-03-25|1.0674|1.1547|1.0442|1.0963| 7.470805297E7|2021|    3|             1341|\n",
      "| ADA|2021-03-24|1.1178|1.1686|  1.04|1.0674| 6.046568588E7|2021|    3|             1342|\n",
      "| ADA|2021-03-23|1.1027|1.1742| 1.082|1.1178| 6.371462798E7|2021|    3|             1343|\n",
      "| ADA|2021-03-22|1.1885|1.2076|  1.07|1.1027| 8.024998154E7|2021|    3|             1344|\n",
      "| ADA|2021-03-21|1.2013|1.2463|1.1607|1.1885| 5.559264307E7|2021|    3|             1345|\n",
      "| ADA|2021-03-20|1.3076| 1.345|1.2001|1.2013|1.2144203052E8|2021|    3|             1346|\n",
      "| ADA|2021-03-19|  1.23| 1.396|1.1808|1.3076|1.6665850755E8|2021|    3|             1347|\n",
      "| ADA|2021-03-18|1.6551|  1.88|1.2155|  1.23|1.1238352869E8|2021|    3|             1348|\n",
      "+----+----------+------+------+------+------+--------------+----+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"gs://crypto-historical-data-2/ver2/ADA/2021/03/data.parquet\"\n",
    "df = spark.read.parquet(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('BASE', StringType(), True), StructField('DATE', StringType(), True), StructField('OPEN', DoubleType(), True), StructField('HIGH', DoubleType(), True), StructField('LOW', DoubleType(), True), StructField('CLOSE', DoubleType(), True), StructField('VOLUME', DoubleType(), True), StructField('YEAR', IntegerType(), True), StructField('MONTH', IntegerType(), True), StructField('__index_level_0__', LongType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|               DATE|   CLOSE|            SMA_5|           SMA_10|           SMA_20|           SMA_50|          SMA_100|          SMA_200|\n",
      "+-------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-13 00:00:00| 99897.8|         99035.98|         99196.71|97502.35500000001|86719.17599999996|74509.28739999999|       68734.2753|\n",
      "|2024-12-12 00:00:00|100050.1|         99290.42|98798.68000000001|97458.66650000002|86053.24219999996|       74084.9978|68577.15489999998|\n",
      "|2024-12-11 00:00:00|101177.0|         99260.38|          98379.0|97378.11600000001|85399.84179999998|       73675.8951|68423.35274999999|\n",
      "|2024-12-10 00:00:00| 96655.1|98999.55999999998|97907.84199999999|       97038.9235|84723.22479999997|       73237.1151|68260.20674999998|\n",
      "|2024-12-09 00:00:00| 97399.9|          99086.0|        97991.388|        96827.963|84170.41999999998|       72860.2478|68116.63349999998|\n",
      "|2024-12-08 00:00:00|101170.0|99357.44000000002|97817.95100000002|        96483.983|83589.69659999997|       72477.3765|67975.18364999998|\n",
      "|2024-12-07 00:00:00| 99899.9|98306.94000000002|         97296.07|       95919.8175|82934.81879999996|       72059.3212|67820.04784999997|\n",
      "|2024-12-06 00:00:00| 99872.9|         97497.62|96499.03400000001|95456.63450000001|82284.86739999997|        71650.781|67677.70919999997|\n",
      "|2024-12-05 00:00:00| 97087.3|96816.12399999998|95811.74500000001|       95015.9075|81639.57819999997|       71246.4484|67509.65279999998|\n",
      "|2024-12-04 00:00:00| 98757.1|        96896.776|95905.83300000001|94528.56250000001|81038.96419999997|       70903.9754|67358.83564999996|\n",
      "|2024-12-03 00:00:00| 95917.5|        96278.462|95808.00000000001|94115.39850000001|80385.13519999998|       70558.9237|67200.28179999997|\n",
      "|2024-12-02 00:00:00| 95853.3|          96285.2|96118.65300000002|        93721.295|79723.69119999999|        70241.545|67046.95519999998|\n",
      "|2024-11-30 00:00:00|96465.42|        95500.448|96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29 00:00:00|97490.56|94807.36600000001|        96170.005|92565.34150000001|78391.44119999999|       69563.0579|66724.28424999998|\n",
      "|2024-11-28 00:00:00|95665.53|94914.88999999998|95664.53800000002|       91526.6165|77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27 00:00:00|95951.19|        95337.538|95150.01499999998|       90571.6765|76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26 00:00:00|91929.54|        95952.106|94543.56500000002|        89570.117|76268.89219999999|       68468.3205|66204.74794999998|\n",
      "|2024-11-25 00:00:00|93000.01|        97254.016|94414.23500000002|       88755.9675|75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24 00:00:00|98028.18|        97532.644|94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "|2024-11-23 00:00:00|97778.77|96414.18600000002|        93151.292|86063.96849999999|74350.98619999998|67406.98700000002|65715.11214999999|\n",
      "+-------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "gcs_jar_path = os.path.abspath(\"config/gcs-connector-hadoop3-latest.jar\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"hehee\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", \"config/key/btcanalysishust-495a3a227f22.json\") \\\n",
    "#     .config(\"spark.jars\", gcs_jar_path) \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,io.delta:delta-core_2.12:2.2.0\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "column_names = [\"BTC\", \"ETH\", \"USDT\", \"USDC\", \"XRP\", \"ADA\", \"DOGE\", \"MATIC\", \"SOL\"]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),  \n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"OPEN\", DoubleType(), True),\n",
    "    StructField(\"HIGH\", DoubleType(), True),\n",
    "    StructField(\"LOW\", DoubleType(), True),\n",
    "    StructField(\"CLOSE\", DoubleType(), True),\n",
    "    StructField(\"VOLUME\", DoubleType(), True),\n",
    "    StructField(\"YEAR\", IntegerType(), True),\n",
    "    StructField(\"MONTH\", IntegerType(), True),\n",
    "    StructField(\"__index_level_0__\", LongType(), True)\n",
    "])\n",
    "\n",
    "import datetime\n",
    "\n",
    "def read_historical_data(coin):\n",
    "    current_year = datetime.datetime.now().year  \n",
    "    years = [str(year) for year in range(2021, current_year + 1)]  \n",
    "\n",
    "    paths = [f\"gs://crypto-historical-data-2/ver2/{coin}/{year}/*\" for year in years]\n",
    "    \n",
    "    return (\n",
    "        spark.read.schema(schema).parquet(*paths)  \n",
    "        .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    "    )\n",
    "\n",
    "def process_coin(coin):\n",
    "    historical_data_df = read_historical_data(coin)\n",
    "    window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    \n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_5\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-4, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_10\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-9, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_20\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-19, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_50\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-49, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_100\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-99, 0)))\n",
    "    historical_data_df = historical_data_df.withColumn(f\"SMA_200\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-199, 0)))\n",
    "\n",
    "    historical_data_df = historical_data_df.select(\"DATE\", \"CLOSE\", \"SMA_5\", \"SMA_10\", \"SMA_20\", \"SMA_50\", \"SMA_100\", \"SMA_200\").orderBy(\"DATE\", ascending=False)\n",
    "    return historical_data_df\n",
    "    \n",
    "    # tmp_dir = f\"gs://indicator-crypto/sma_results/batch/{coin}\"\n",
    "\n",
    "    # historical_data_df.write \\\n",
    "    #     .format(\"csv\") \\\n",
    "    #     .option(\"header\", \"true\") \\\n",
    "    #     .option(\"path\", tmp_dir) \\\n",
    "    #     .mode(\"append\") \\\n",
    "    #     .save()\n",
    "\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = [executor.submit(process_coin, coin) for coin in column_names]\n",
    "#     for future in as_completed(futures):\n",
    "#         print(future.result())\n",
    "\n",
    "coin = column_names[0]\n",
    "process_coin(coin).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n",
      "DataFrame[DATE: timestamp, CLOSE: double, EMA_5: double, EMA_10: double, EMA_20: double, EMA_50: double, EMA_100: double, EMA_200: double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Danh sách các cột coin\n",
    "column_names = [\"BTC\", \"ETH\", \"USDT\", \"USDC\", \"XRP\", \"ADA\", \"DOGE\", \"MATIC\", \"SOL\"]\n",
    "\n",
    "# Schema của dữ liệu đầu vào\n",
    "schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),  \n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"OPEN\", DoubleType(), True),\n",
    "    StructField(\"HIGH\", DoubleType(), True),\n",
    "    StructField(\"LOW\", DoubleType(), True),\n",
    "    StructField(\"CLOSE\", DoubleType(), True),\n",
    "    StructField(\"VOLUME\", DoubleType(), True),\n",
    "    StructField(\"YEAR\", IntegerType(), True),\n",
    "    StructField(\"MONTH\", IntegerType(), True),\n",
    "    StructField(\"__index_level_0__\", LongType(), True)\n",
    "])\n",
    "\n",
    "def read_historical_data(coin):\n",
    "    current_year = datetime.datetime.now().year  \n",
    "    years = [str(year) for year in range(2021, current_year + 1)]  \n",
    "\n",
    "    paths = [f\"gs://crypto-historical-data-2/ver2/{coin}/{year}/*\" for year in years]\n",
    "    \n",
    "    return (\n",
    "        spark.read.schema(schema).parquet(*paths)  \n",
    "        .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Hàm Python tính EMA\n",
    "def calculate_ema_udf(period):\n",
    "    alpha = 2 / (period + 1)\n",
    "    \n",
    "    def calculate_ema(values):\n",
    "        ema_values = []\n",
    "        for i, close in enumerate(values):\n",
    "            if i == 0:\n",
    "                ema_values.append(close)  # Giá trị EMA đầu tiên là CLOSE đầu tiên\n",
    "            else:\n",
    "                ema = alpha * close + (1 - alpha) * ema_values[-1]\n",
    "                ema_values.append(ema)\n",
    "        return round(ema_values[-1], 2)  # Trả về giá trị EMA cuối cùng trong danh sách\n",
    "\n",
    "    return udf(lambda values: calculate_ema(values), DoubleType())\n",
    "\n",
    "# Thêm cột EMA bằng UDF\n",
    "def calculate_ema(df, column, period):\n",
    "    window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    df = df.withColumn(f\"{column}_LIST\", F.collect_list(column).over(window_spec))\n",
    "    ema_udf = calculate_ema_udf(period)\n",
    "    return df.withColumn(f\"EMA_{period}\", ema_udf(F.col(f\"{column}_LIST\"))).drop(f\"{column}_LIST\")\n",
    "\n",
    "\n",
    "def process_coin(coin):\n",
    "    historical_data_df = read_historical_data(coin)\n",
    "\n",
    "    # Tính toán EMA cho các khoảng thời gian\n",
    "    ema_periods = [5, 10, 20, 50, 100, 200]\n",
    "    for period in ema_periods:\n",
    "        historical_data_df = calculate_ema(historical_data_df, \"CLOSE\", period)\n",
    "\n",
    "    historical_data_df = historical_data_df.select(\"DATE\", \"CLOSE\", *[f\"EMA_{period}\" for period in ema_periods]).orderBy(\"DATE\", ascending=False)\n",
    "\n",
    "\n",
    "    tmp_dir = f\"gs://indicator-crypto/ema_results/batch/{coin}\"\n",
    "\n",
    "    historical_data_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"path\", tmp_dir) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "    return historical_data_df\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_coin, coin) for coin in column_names]\n",
    "    for future in as_completed(futures):\n",
    "        print(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Explicitly set Python and library paths\n",
    "python_path = sys.executable\n",
    "spark_home = r\"D:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\"  # Adjust to your Spark installation path\n",
    "hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "java_home = r\"D:\\Empty\\Java\"\n",
    "\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "bucket_name = 'ba_bucket3'\n",
    "gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "# Add Spark and Python paths\n",
    "sys.path.append(os.path.join(spark_home, 'python'))\n",
    "sys.path.append(os.path.join(spark_home, 'python', 'lib', 'py4j-0.10.9.7-src.zip'))\n",
    "\n",
    "# spark = get_spark_session()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "# from config import *\n",
    "\n",
    "# def get_spark_session():\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"GCS Connector Test\") \\\n",
    "#     .config(\"spark.pyspark.python\", python_path) \\\n",
    "#     .config(\"spark.pyspark.driver.python\", python_path) \\\n",
    "#     .config(\"spark.driver.extraLibraryPath\", os.path.dirname(python_path)) \\\n",
    "#     .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "#     .config(\"spark.local.dir\", \"D:/Empty/spark-temp\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,io.delta:delta-core_2.12:2.2.0\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"4g\") \\\n",
    "#     .config(\"spark.python.worker.memory\", \"2g\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "            \n",
    "    # return spark\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"GCS Connector Test\") \\\n",
    "#     .config(\"spark.pyspark.python\", python_path) \\\n",
    "#     .config(\"spark.pyspark.driver.python\", python_path) \\\n",
    "#     .config(\"spark.driver.extraLibraryPath\", os.path.dirname(python_path)) \\\n",
    "#     .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "#     .config(\"spark.local.dir\", \"D:/Empty/spark-temp\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,\"\n",
    "#             \"io.delta:delta-core_2.12:2.2.0,\"\n",
    "#             \"io.delta:delta-spark_2.12:2.2.0\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"4g\") \\\n",
    "#     .config(\"spark.python.worker.memory\", \"2g\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"hehee\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", \"config/key/btcanalysishust-495a3a227f22.json\") \\\n",
    "    .config(\"spark.jars\", gcs_connector_jar) \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.3,io.delta:delta-core_2.13:2.4.0\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinimalTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o273.load.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.delta.sources.DeltaDataSource Unable to get public no-arg constructor\r\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:582)\r\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:673)\r\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1233)\r\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1265)\r\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)\r\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)\r\n\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:37)\r\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\r\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\r\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:58)\r\n\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\r\n\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\r\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:158)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce\r\n\tat java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\r\n\tat java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3137)\r\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3342)\r\n\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\r\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:660)\r\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:657)\r\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:551)\r\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:668)\r\n\t... 26 more\r\nCaused by: java.lang.ClassNotFoundException: scala.collection.GenTraversableOnce\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 53\u001b[0m\n\u001b[0;32m     29\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     30\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[0;32m     31\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprices\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType([\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     ]))\n\u001b[0;32m     50\u001b[0m ])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Đọc dữ liệu từ Kafka\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m kafka_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m35.206.252.44:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrypto-pricessss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxOffsetsPerTrigger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m parsed_df \u001b[38;5;241m=\u001b[39m kafka_df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mfrom_json(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m), schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     64\u001b[0m crypto_parsed_df \u001b[38;5;241m=\u001b[39m parsed_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     65\u001b[0m     F\u001b[38;5;241m.\u001b[39mto_timestamp(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHH:mm:ss.SSSSSSXXX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     66\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.prices.bitcoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBTC\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.prices.solana\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m )\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\sql\\streaming\\readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\Empty\\spark-3.5.3-bin-hadoop3-scala2.13\\spark-3.5.3-bin-hadoop3-scala2.13\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o273.load.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.delta.sources.DeltaDataSource Unable to get public no-arg constructor\r\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:582)\r\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:673)\r\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1233)\r\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1265)\r\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)\r\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)\r\n\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:37)\r\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\r\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\r\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:58)\r\n\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\r\n\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\r\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:158)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce\r\n\tat java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\r\n\tat java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3137)\r\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3342)\r\n\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\r\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:660)\r\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:657)\r\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:551)\r\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:668)\r\n\t... 26 more\r\nCaused by: java.lang.ClassNotFoundException: scala.collection.GenTraversableOnce\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "# from delta import *\n",
    "# from delta.tables import *\n",
    "\n",
    "\n",
    "# gcs_jar_path = os.path.abspath(\"config/gcs-connector-hadoop3-latest.jar\")\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Khởi tạo SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"hehee\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "#     .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", \"config/key/btcanalysishust-495a3a227f22.json\") \\\n",
    "#     .config(\"spark.jars\", gcs_jar_path) \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,io.delta:delta-core_2.12:2.2.0\") \\\n",
    "#     .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Schema của dữ liệu Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-pricessss\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(F.col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "crypto_parsed_df = parsed_df.select(\n",
    "    F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "    F.col(\"data.prices.bitcoin\").alias(\"BTC\"),\n",
    "    F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "    F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "    F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "    F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "    F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "    F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "    F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "    F.col(\"data.prices.solana\").alias(\"SOL\")\n",
    ")\n",
    "crypto_parsed_df.show()\n",
    "\n",
    "# column_names = [col for col in crypto_parsed_df.columns if col != 'DATE']\n",
    "\n",
    "# historical_schema = StructType([\n",
    "#     StructField(\"BASE\", StringType(), True),  \n",
    "#     StructField(\"DATE\", StringType(), True),\n",
    "#     StructField(\"OPEN\", DoubleType(), True),\n",
    "#     StructField(\"HIGH\", DoubleType(), True),\n",
    "#     StructField(\"LOW\", DoubleType(), True),\n",
    "#     StructField(\"CLOSE\", DoubleType(), True),\n",
    "#     StructField(\"VOLUME\", DoubleType(), True),\n",
    "#     StructField(\"YEAR\", IntegerType(), True),\n",
    "#     StructField(\"MONTH\", IntegerType(), True),\n",
    "#     StructField(\"__index_level_0__\", LongType(), True)\n",
    "# ])\n",
    "\n",
    "\n",
    "# def read_historical_data(coin):\n",
    "#     return (\n",
    "#         spark.read.schema(historical_schema).format(\"parquet\")\n",
    "#         .load(f\"gs://crypto-historical-data-2/ver2/{coin}/2024/*\")\n",
    "#         .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    "#     )\n",
    "\n",
    "# def process_coin(coin, micro_batch_latest_df):\n",
    "#     historical_data_df = read_historical_data(coin)\n",
    "#     micro_batch = micro_batch_latest_df.select(\"DATE\", coin).withColumnRenamed(coin, \"CLOSE\")\n",
    "#     combined_df = micro_batch.unionByName(historical_data_df)\n",
    "\n",
    "#     window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_5\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-4, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_10\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-9, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_20\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-19, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_50\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-49, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_100\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-99, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_200\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-199, 0)))\n",
    "\n",
    "#     combined_df = combined_df.orderBy(\"DATE\", ascending=False)\n",
    "    \n",
    "#     current_date = F.current_date() \n",
    "#     combined_df = combined_df.filter(F.col(\"DATE\") >= current_date)\n",
    "\n",
    "#     tmp_dir = f\"gs://indicator-crypto/sma_results/tmp/{coin}\"\n",
    "\n",
    "#     #combined_df.write.format(\"console\").option(\"truncate\", False).save()\n",
    "#     # combined_df.write \\\n",
    "#     #     .format(\"csv\") \\\n",
    "#     #     .option(\"header\", \"true\") \\\n",
    "#     #     .option(\"path\", tmp_dir) \\\n",
    "#     #     .mode(\"append\") \\\n",
    "#     #     .save()\n",
    "#     return combined_df\n",
    "    \n",
    "# def process_batch(micro_batch_df, batch_id):\n",
    "#     micro_batch_latest_df = (\n",
    "#         micro_batch_df\n",
    "#         .withColumn(\"row_num\", F.row_number().over(Window.orderBy(F.col(\"DATE\").desc())))\n",
    "#         .filter(F.col(\"row_num\") == 1)\n",
    "#         .drop(\"row_num\")\n",
    "#     )\n",
    "    \n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(process_coin, coin, micro_batch_latest_df) for coin in column_names]\n",
    "#         for future in as_completed(futures):\n",
    "#             print(future.result()) \n",
    "\n",
    "# # Thực thi stream\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .foreachBatch(process_batch) \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "+----+---+---+----+----+---+---+----+-----+---+\n",
      "|DATE|BTC|ETH|USDT|USDC|XRP|ADA|DOGE|MATIC|SOL|\n",
      "+----+---+---+----+----+---+---+----+-----+---+\n",
      "+----+---+---+----+----+---+---+----+-----+---+\n",
      "\n",
      "Processing batch 1\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:38:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 2\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:38:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 3\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:38:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 4\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:38:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 5\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:39:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 6\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:40:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 7\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:41:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 8\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|   ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 19:42:...|101350.0|3876.9|0.999873|0.999313|2.42|1.087|0.402838|0.60537|222.74|\n",
      "+--------------------+--------+------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 9\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:43:...|101372.0|3877.59| 1.0|0.999838|2.42|1.089|0.402514|0.604863|222.76|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 10\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:44:...|101372.0|3877.59| 1.0|0.999838|2.42|1.089|0.402514|0.604863|222.76|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 11\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:45:...|101372.0|3877.59| 1.0|0.999838|2.42|1.089|0.402514|0.604863|222.76|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 12\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:54:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 13\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:54:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 14\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:54:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 15\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:54:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 16\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:55:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 17\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:56:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 18\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:57:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 19\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:58:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 20\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 19:59:...|101344.0|3876.22| 1.0|0.999907|2.42|1.086|0.402219|0.603807|222.47|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 21\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:00:...|101280.0|3871.45|0.999809|0.999717|2.41|1.085|0.401329|0.602376|222.31|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 22\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:01:...|101280.0|3871.45|0.999809|0.999717|2.41|1.085|0.401329|0.602376|222.31|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 23\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:02:...|101280.0|3871.45|0.999809|0.999717|2.41|1.085|0.401329|0.602376|222.31|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 24\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:03:...|101280.0|3871.45|0.999809|0.999717|2.41|1.085|0.401329|0.602376|222.31|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 25\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:04:...|101280.0|3871.45|0.999809|0.999717|2.41|1.085|0.401329|0.602376|222.31|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 26\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:05:...|101280.0|3871.45|0.999809|0.999717|2.41|1.085|0.401329|0.602376|222.31|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 27\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:06:...|101372.0|3873.55|1.001|1.001|2.41|1.085|0.401844|0.602142|222.37|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 28\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:07:...|101372.0|3873.55|1.001|1.001|2.41|1.085|0.401844|0.602142|222.37|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 29\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:08:...|101372.0|3873.55|1.001|1.001|2.41|1.085|0.401844|0.602142|222.37|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 30\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:09:...|101372.0|3873.55|1.001|1.001|2.41|1.085|0.401844|0.602142|222.37|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 31\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:10:...|101372.0|3873.55|1.001|1.001|2.41|1.085|0.401844|0.602142|222.37|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 32\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:11:...|101372.0|3873.55|1.001|1.001|2.41|1.085|0.401844|0.602142|222.37|\n",
      "+--------------------+--------+-------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 33\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:12:...|101377.0|3871.89|0.999924|0.999915|2.41|1.083|0.401554|0.60236|222.18|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 34\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:13:...|101377.0|3871.89|0.999924|0.999915|2.41|1.083|0.401554|0.60236|222.18|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 35\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:14:...|101377.0|3871.89|0.999924|0.999915|2.41|1.083|0.401554|0.60236|222.18|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 36\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:15:...|101377.0|3871.89|0.999924|0.999915|2.41|1.083|0.401554|0.60236|222.18|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 37\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:16:...|101377.0|3871.89|0.999924|0.999915|2.41|1.083|0.401554|0.60236|222.18|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 38\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:17:...|101377.0|3871.89|0.999924|0.999915|2.41|1.083|0.401554|0.60236|222.18|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 39\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC|XRP| ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|2024-12-14 20:18:...|101198.0|3864.95|0.999525|0.999357|2.4|1.08|0.400167|0.599456|221.66|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "\n",
      "Processing batch 40\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC|XRP| ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|2024-12-14 20:19:...|101198.0|3864.95|0.999525|0.999357|2.4|1.08|0.400167|0.599456|221.66|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "\n",
      "Processing batch 41\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC|XRP| ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|2024-12-14 20:20:...|101198.0|3864.95|0.999525|0.999357|2.4|1.08|0.400167|0.599456|221.66|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "\n",
      "Processing batch 42\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC|XRP| ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|2024-12-14 20:21:...|101198.0|3864.95|0.999525|0.999357|2.4|1.08|0.400167|0.599456|221.66|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "\n",
      "Processing batch 43\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC|XRP| ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|2024-12-14 20:22:...|101198.0|3864.95|0.999525|0.999357|2.4|1.08|0.400167|0.599456|221.66|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "\n",
      "Processing batch 44\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC|XRP| ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "|2024-12-14 20:23:...|101198.0|3864.95|0.999525|0.999357|2.4|1.08|0.400167|0.599456|221.66|\n",
      "+--------------------+--------+-------+--------+--------+---+----+--------+--------+------+\n",
      "\n",
      "Processing batch 45\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:24:...|100874.0|3850.67|0.998623|0.998624|2.38|1.071|0.396603|0.593723|220.81|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 46\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:25:...|100874.0|3850.67|0.998623|0.998624|2.38|1.071|0.396603|0.593723|220.81|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 47\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:26:...|100874.0|3850.67|0.998623|0.998624|2.38|1.071|0.396603|0.593723|220.81|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 48\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:27:...|100874.0|3850.67|0.998623|0.998624|2.38|1.071|0.396603|0.593723|220.81|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 49\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:28:...|100874.0|3850.67|0.998623|0.998624|2.38|1.071|0.396603|0.593723|220.81|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 50\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:29:...|100874.0|3850.67|0.998623|0.998624|2.38|1.071|0.396603|0.593723|220.81|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 51\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:30:...|101043.0|3859.45|0.999995|0.999134|2.39|1.073|0.397747|0.59743|221.44|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 52\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:31:...|101043.0|3859.45|0.999995|0.999134|2.39|1.073|0.397747|0.59743|221.44|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 53\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:32:...|101043.0|3859.45|0.999995|0.999134|2.39|1.073|0.397747|0.59743|221.44|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 54\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:33:...|101043.0|3859.45|0.999995|0.999134|2.39|1.073|0.397747|0.59743|221.44|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 55\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:34:...|101043.0|3859.45|0.999995|0.999134|2.39|1.073|0.397747|0.59743|221.44|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 56\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|    USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:35:...|101043.0|3859.45|0.999995|0.999134|2.39|1.073|0.397747|0.59743|221.44|\n",
      "+--------------------+--------+-------+--------+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 57\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:36:...|101174.0|3865.28| 1.0| 1.0|2.39|1.075|0.397614|0.598008|221.82|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 58\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:37:...|101174.0|3865.28| 1.0| 1.0|2.39|1.075|0.397614|0.598008|221.82|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 59\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:38:...|101174.0|3865.28| 1.0| 1.0|2.39|1.075|0.397614|0.598008|221.82|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 60\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:39:...|101174.0|3865.28| 1.0| 1.0|2.39|1.075|0.397614|0.598008|221.82|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 61\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:40:...|101174.0|3865.28| 1.0| 1.0|2.39|1.075|0.397614|0.598008|221.82|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 62\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:41:...|101174.0|3865.28| 1.0| 1.0|2.39|1.075|0.397614|0.598008|221.82|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 63\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|   ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:42:...|101249.0|3868.3|1.001|1.001|2.39|1.076|0.398221|0.598581|222.18|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 64\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|   ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:43:...|101249.0|3868.3|1.001|1.001|2.39|1.076|0.398221|0.598581|222.18|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 65\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|   ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:44:...|101249.0|3868.3|1.001|1.001|2.39|1.076|0.398221|0.598581|222.18|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 66\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|   ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:45:...|101249.0|3868.3|1.001|1.001|2.39|1.076|0.398221|0.598581|222.18|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 67\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|   ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:46:...|101249.0|3868.3|1.001|1.001|2.39|1.076|0.398221|0.598581|222.18|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 68\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|   ETH| USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:47:...|101249.0|3868.3|1.001|1.001|2.39|1.076|0.398221|0.598581|222.18|\n",
      "+--------------------+--------+------+-----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 69\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:48:...|101261.0|3870.81| 1.0| 1.0|2.39|1.075|0.397806|0.598834|222.25|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 70\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:50:...|101261.0|3870.81| 1.0| 1.0|2.39|1.075|0.397806|0.598834|222.25|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 71\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:50:...|101261.0|3870.81| 1.0| 1.0|2.39|1.075|0.397806|0.598834|222.25|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 72\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:51:...|101261.0|3870.81| 1.0| 1.0|2.39|1.075|0.397806|0.598834|222.25|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 73\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:52:...|101261.0|3870.81| 1.0| 1.0|2.39|1.075|0.397806|0.598834|222.25|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 74\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 20:53:...|101261.0|3870.81| 1.0| 1.0|2.39|1.075|0.397806|0.598834|222.25|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 75\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:54:...|101143.0|3865.21| 1.0|0.999903|2.38|1.073|0.396388|0.59709|221.67|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 76\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:55:...|101143.0|3865.21| 1.0|0.999903|2.38|1.073|0.396388|0.59709|221.67|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 77\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:56:...|101143.0|3865.21| 1.0|0.999903|2.38|1.073|0.396388|0.59709|221.67|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 78\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:57:...|101143.0|3865.21| 1.0|0.999903|2.38|1.073|0.396388|0.59709|221.67|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 79\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:58:...|101143.0|3865.21| 1.0|0.999903|2.38|1.073|0.396388|0.59709|221.67|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 80\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|  MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "|2024-12-14 20:59:...|101143.0|3865.21| 1.0|0.999903|2.38|1.073|0.396388|0.59709|221.67|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+-------+------+\n",
      "\n",
      "Processing batch 81\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:00:...|101161.0|3862.88| 1.0|0.999916|2.38|1.072|0.395447|0.596959|221.21|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 82\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:01:...|101161.0|3862.88| 1.0|0.999916|2.38|1.072|0.395447|0.596959|221.21|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 83\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:02:...|101161.0|3862.88| 1.0|0.999916|2.38|1.072|0.395447|0.596959|221.21|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 84\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:03:...|101161.0|3862.88| 1.0|0.999916|2.38|1.072|0.395447|0.596959|221.21|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 85\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:04:...|101161.0|3862.88| 1.0|0.999916|2.38|1.072|0.395447|0.596959|221.21|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 86\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|    USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:05:...|101161.0|3862.88| 1.0|0.999916|2.38|1.072|0.395447|0.596959|221.21|\n",
      "+--------------------+--------+-------+----+--------+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 87\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:06:...|101380.0|3874.12| 1.0|1.001|2.39|1.076|0.396958|0.599652|221.88|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 88\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:07:...|101380.0|3874.12| 1.0|1.001|2.39|1.076|0.396958|0.599652|221.88|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 89\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:08:...|101380.0|3874.12| 1.0|1.001|2.39|1.076|0.396958|0.599652|221.88|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 90\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:09:...|101380.0|3874.12| 1.0|1.001|2.39|1.076|0.396958|0.599652|221.88|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 91\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:10:...|101380.0|3874.12| 1.0|1.001|2.39|1.076|0.396958|0.599652|221.88|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 92\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT| USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:11:...|101380.0|3874.12| 1.0|1.001|2.39|1.076|0.396958|0.599652|221.88|\n",
      "+--------------------+--------+-------+----+-----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 93\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:12:...|101297.0|3873.99| 1.0| 1.0|2.39|1.075|0.396444|0.599967|221.43|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n",
      "Processing batch 94\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|                DATE|     BTC|    ETH|USDT|USDC| XRP|  ADA|    DOGE|   MATIC|   SOL|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "|2024-12-14 21:13:...|101297.0|3873.99| 1.0| 1.0|2.39|1.075|0.396444|0.599967|221.43|\n",
      "+--------------------+--------+-------+----+----+----+-----+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "# from delta import *\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToSpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-pricessss\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "# Parse dữ liệu JSON\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "# Select relevant columns for cryptocurrencies\n",
    "crypto_parsed_df = parsed_df.select(\n",
    "    F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "    F.col(\"data.prices.bitcoin\").alias(\"BTC\"),\n",
    "    F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "    F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "    F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "    F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "    F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "    F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "    F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "    F.col(\"data.prices.solana\").alias(\"SOL\")\n",
    ")\n",
    "\n",
    "\n",
    "# Xuất ra console\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .start()\n",
    "\n",
    "# # query.awaitTermination()  # Đảm bảo stream chạy liên tục\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\  # Chạy mỗi 10 giây\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    print(f\"Processing batch {epoch_id}\")\n",
    "    df.show()  # Hoặc thực hiện các hành động khác\n",
    "\n",
    "query = crypto_parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "+----+---+---+----+----+---+---+----+-----+---+\n",
      "|DATE|BTC|ETH|USDT|USDC|XRP|ADA|DOGE|MATIC|SOL|\n",
      "+----+---+---+----+----+---+---+----+-----+---+\n",
      "+----+---+---+----+----+---+---+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "# from delta import *\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToSpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-pricessss\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "# Parse dữ liệu JSON\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "# Select relevant columns for cryptocurrencies\n",
    "crypto_parsed_df = parsed_df.select(\n",
    "    F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "    F.col(\"data.prices.bitcoin\").alias(\"BTC\"),\n",
    "    F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "    F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "    F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "    F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "    F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "    F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "    F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "    F.col(\"data.prices.solana\").alias(\"SOL\")\n",
    ")\n",
    "\n",
    "\n",
    "# Xuất ra console\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .start()\n",
    "\n",
    "# # query.awaitTermination()  # Đảm bảo stream chạy liên tục\n",
    "# query = crypto_parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\  # Chạy mỗi 10 giây\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    print(f\"Processing batch {epoch_id}\")\n",
    "    df.show()  # Hoặc thực hiện các hành động khác\n",
    "\n",
    "query = crypto_parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = crypto_parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\  # Cập nhật trigger để chạy mỗi 10 giây\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "# gcs_jar_path = os.path.abspath(\"config/gcs-connector-hadoop3-latest.jar\")\n",
    "from pyspark.sql import SparkSession\n",
    "hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"hehee\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .config(\"spark.jars\", gcs_connector_jar) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.3\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema của dữ liệu Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-pricessss\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "# kafka_df.show()\n",
    "\n",
    "# parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "#     .select(F.from_json(F.col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "# crypto_parsed_df = parsed_df.select(\n",
    "#     F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "#     F.col(\"data.prices.bitcoin\").alias(\"BTC\"),\n",
    "#     F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "#     F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "#     F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "#     F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "#     F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "#     F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "#     F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "#     F.col(\"data.prices.solana\").alias(\"SOL\")\n",
    "# )\n",
    "\n",
    "# column_names = [col for col in crypto_parsed_df.columns if col != 'DATE']\n",
    "\n",
    "# historical_schema = StructType([\n",
    "#     StructField(\"BASE\", StringType(), True),  \n",
    "#     StructField(\"DATE\", StringType(), True),\n",
    "#     StructField(\"OPEN\", DoubleType(), True),\n",
    "#     StructField(\"HIGH\", DoubleType(), True),\n",
    "#     StructField(\"LOW\", DoubleType(), True),\n",
    "#     StructField(\"CLOSE\", DoubleType(), True),\n",
    "#     StructField(\"VOLUME\", DoubleType(), True),\n",
    "#     StructField(\"YEAR\", IntegerType(), True),\n",
    "#     StructField(\"MONTH\", IntegerType(), True),\n",
    "#     StructField(\"__index_level_0__\", LongType(), True)\n",
    "# ])\n",
    "\n",
    "\n",
    "# def read_historical_data(coin):\n",
    "#     return (\n",
    "#         spark.read.schema(historical_schema).format(\"parquet\")\n",
    "#         .load(f\"gs://crypto-historical-data-2/ver2/{coin}/2024/*\")\n",
    "#         .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    "#     )\n",
    "\n",
    "# def process_coin(coin, micro_batch_latest_df):\n",
    "#     historical_data_df = read_historical_data(coin)\n",
    "#     micro_batch = micro_batch_latest_df.select(\"DATE\", coin).withColumnRenamed(coin, \"CLOSE\")\n",
    "#     combined_df = micro_batch.unionByName(historical_data_df)\n",
    "\n",
    "#     window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_5\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-4, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_10\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-9, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_20\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-19, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_50\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-49, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_100\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-99, 0)))\n",
    "#     combined_df = combined_df.withColumn(f\"SMA_200\", F.avg(F.col(\"CLOSE\")).over(window_spec.rowsBetween(-199, 0)))\n",
    "\n",
    "#     combined_df = combined_df.orderBy(\"DATE\", ascending=False)\n",
    "    \n",
    "#     current_date = F.current_date() \n",
    "#     combined_df = combined_df.filter(F.col(\"DATE\") >= current_date)\n",
    "\n",
    "#     # tmp_dir = f\"gs://indicator-crypto/sma_results/tmp/{coin}\"\n",
    "\n",
    "#     # #combined_df.write.format(\"console\").option(\"truncate\", False).save()\n",
    "#     # combined_df.write \\\n",
    "#     #     .format(\"csv\") \\\n",
    "#     #     .option(\"header\", \"true\") \\\n",
    "#     #     .option(\"path\", tmp_dir) \\\n",
    "#     #     .mode(\"append\") \\\n",
    "#     #     .save()\n",
    "#     return combined_df\n",
    "    \n",
    "# def process_batch(micro_batch_df, batch_id):\n",
    "#     micro_batch_latest_df = (\n",
    "#         micro_batch_df\n",
    "#         .withColumn(\"row_num\", F.row_number().over(Window.orderBy(F.col(\"DATE\").desc())))\n",
    "#         .filter(F.col(\"row_num\") == 1)\n",
    "#         .drop(\"row_num\")\n",
    "#     )\n",
    "#     # coin = column_names[0]\n",
    "#     # process_coin(coin, micro_batch_latest_df).show()\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(process_coin, coin, micro_batch_latest_df) for coin in column_names]\n",
    "#         for future in as_completed(futures):\n",
    "#             print(future.result()) \n",
    "#             result_df = future.result()  # Processed DataFrame for each coin\n",
    "#             result_df.show(truncate=False)  # Display the DataFrame without truncating content\n",
    "            \n",
    "            \n",
    "# # process_batch(crypto_parsed_df).show()\n",
    "# # Thực thi stream\n",
    "def test(micro_batch_df, batch_id):\n",
    "    # In dữ liệu của micro_batch_df\n",
    "    micro_batch_df.show(truncate=False)\n",
    "\n",
    "# Thực thi stream và chỉ in dữ liệu\n",
    "query = kafka_df.writeStream \\\n",
    "    .foreachBatch(test) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# query = kafka_df.writeStream \\\n",
    "#     .foreachBatch(test) \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "hadoop_home = r\"D:\\Empty\\hadoop\\hadoop\"\n",
    "KEY_FILE = r\"D:\\Empty\\btcanalysishust-d7c3a4830bef.json\"\n",
    "gcs_connector_jar = os.path.join(hadoop_home, 'share', 'hadoop', 'tools', 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "\n",
    "# def get_spark_session():\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Connector Test\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.executor.extraClassPath\", gcs_connector_jar) \\\n",
    "    .config(\"spark.local.dir\", \"D:/Empty/spark-temp\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", KEY_FILE) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.3\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "            \n",
    "    \n",
    "# Schema của dữ liệu Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-pricessss\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(F.col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "crypto_parsed_df = parsed_df.select(\n",
    "    F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "    F.col(\"data.prices.bitcoin\").alias(\"BTC\"),\n",
    "    F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "    F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "    F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "    F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "    F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "    F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "    F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "    F.col(\"data.prices.solana\").alias(\"SOL\")\n",
    ")\n",
    "\n",
    "column_names = [col for col in crypto_parsed_df.columns if col != 'DATE']\n",
    "\n",
    "historical_schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),  \n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"OPEN\", DoubleType(), True),\n",
    "    StructField(\"HIGH\", DoubleType(), True),\n",
    "    StructField(\"LOW\", DoubleType(), True),\n",
    "    StructField(\"CLOSE\", DoubleType(), True),\n",
    "    StructField(\"VOLUME\", DoubleType(), True),\n",
    "    StructField(\"YEAR\", IntegerType(), True),\n",
    "    StructField(\"MONTH\", IntegerType(), True),\n",
    "    StructField(\"__index_level_0__\", LongType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "def read_historical_data(coin):\n",
    "    return (\n",
    "        spark.read.schema(historical_schema).format(\"parquet\")\n",
    "        .load(f\"gs://crypto-historical-data-2/ver2/{coin}/2024/*\")\n",
    "        .select(F.col(\"DATE\").cast(\"timestamp\"), \"CLOSE\")\n",
    "    )\n",
    "    \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Hàm tính EMA\n",
    "def calculate_ema(df, column, period):\n",
    "    alpha = 2 / (period + 1)\n",
    "    window_spec = Window.orderBy(\"DATE\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    \n",
    "    # Tính giá trị EMA\n",
    "    ema_column = F.avg(F.col(column).cast(DoubleType())).over(window_spec)\n",
    "    ema_values = df.withColumn(f\"EMA_{period}\", alpha * F.col(column) + (1 - alpha) * ema_column)\n",
    "    print(ema_values)\n",
    "    return ema_values\n",
    "\n",
    "def process_coin(coin, micro_batch_latest_df):\n",
    "    historical_data_df = read_historical_data(coin)\n",
    "    micro_batch = micro_batch_latest_df.select(\"DATE\", coin).withColumnRenamed(coin, \"CLOSE\")\n",
    "    \n",
    "    # Kết hợp dữ liệu hiện tại với dữ liệu lịch sử\n",
    "    combined_df = micro_batch.unionByName(historical_data_df)\n",
    "    \n",
    "    # Tính toán EMA\n",
    "    ema_periods = [5, 10, 20, 50, 100, 200]\n",
    "    for period in ema_periods:\n",
    "        combined_df = calculate_ema(combined_df, \"CLOSE\", period)\n",
    "    \n",
    "    # Sắp xếp và lọc dữ liệu theo ngày\n",
    "    combined_df = combined_df.orderBy(\"DATE\", ascending=False)\n",
    "    current_date = F.current_date() \n",
    "    combined_df = combined_df.filter(F.col(\"DATE\") >= current_date)\n",
    "    \n",
    "    # Lưu kết quả vào GCS\n",
    "    tmp_dir = f\"gs://indicator-crypto/ema_results/tmp/{coin}\"\n",
    "    combined_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"path\", tmp_dir) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "\n",
    "def process_batch(micro_batch_df, batch_id):\n",
    "    micro_batch_latest_df = (\n",
    "        micro_batch_df\n",
    "        .withColumn(\"row_num\", F.row_number().over(Window.orderBy(F.col(\"DATE\").desc())))\n",
    "        .filter(F.col(\"row_num\") == 1)\n",
    "        .drop(\"row_num\")\n",
    "    )\n",
    "    \n",
    "    # Sử dụng ThreadPoolExecutor để xử lý nhiều đồng tiền song song\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_coin, coin, micro_batch_latest_df) for coin in column_names]\n",
    "        for future in as_completed(futures):\n",
    "            print(future.result())\n",
    "\n",
    "# Chạy job streaming\n",
    "query = crypto_parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
