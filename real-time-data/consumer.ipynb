{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, LongType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "keyfile_path = \"btcanalysishust-b10a2ef12088.json\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumer\") \\\n",
    "  .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.type\", \"OAuth2\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.input.close.input.streams.after.task.complete\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", keyfile_path) \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m      2\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[0;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprices\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType([\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     ]))\n\u001b[0;32m     22\u001b[0m ])\n\u001b[0;32m     24\u001b[0m kafka_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m35.206.252.44:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrypto-prices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxOffsetsPerTrigger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m parsed_df \u001b[38;5;241m=\u001b[39m kafka_df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mfrom_json(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m), schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     34\u001b[0m crypto_parsed_df \u001b[38;5;241m=\u001b[39m parsed_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     35\u001b[0m     F\u001b[38;5;241m.\u001b[39mto_timestamp(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHH:mm:ss.SSSSSSXXX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     36\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.prices.bitcoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBTC\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.prices.solana\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m )\n",
      "File \u001b[1;32mE:\\Application\\BigDataSetup\\Spark\\spark-3.5.3\\python\\pyspark\\sql\\streaming\\readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mE:\\Application\\BigDataSetup\\Spark\\spark-3.5.3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mE:\\Application\\BigDataSetup\\Spark\\spark-3.5.3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"prices\", StructType([\n",
    "        StructField(\"bitcoin\", FloatType()),\n",
    "        StructField(\"ethereum\", FloatType()),\n",
    "        StructField(\"tether\", FloatType()),\n",
    "        StructField(\"usd-coin\", FloatType()),\n",
    "        StructField(\"ripple\", FloatType()),\n",
    "        StructField(\"cardano\", FloatType()),\n",
    "        StructField(\"dogecoin\", FloatType()),\n",
    "        StructField(\"matic-network\", FloatType()),\n",
    "        StructField(\"solana\", FloatType()),\n",
    "        StructField(\"litecoin\", FloatType()),\n",
    "        StructField(\"polkadot\", FloatType()),\n",
    "        StructField(\"shiba-inu\", FloatType()),\n",
    "        StructField(\"tron\", FloatType()),\n",
    "        StructField(\"cosmos\", FloatType()),\n",
    "        StructField(\"chainlink\", FloatType()),\n",
    "        StructField(\"stellar\", FloatType()),\n",
    "        StructField(\"near\", FloatType()),\n",
    "    ]))\n",
    "])\n",
    "\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"35.206.252.44:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto-prices\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(F.col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "crypto_parsed_df = parsed_df.select(\n",
    "    F.to_timestamp(F.col(\"data.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSXXX\").alias(\"DATE\"),\n",
    "    F.col(\"data.prices.bitcoin\").alias(\"BTC\"),\n",
    "    F.col(\"data.prices.ethereum\").alias(\"ETH\"),\n",
    "    F.col(\"data.prices.tether\").alias(\"USDT\"),\n",
    "    F.col(\"data.prices.usd-coin\").alias(\"USDC\"),\n",
    "    F.col(\"data.prices.ripple\").alias(\"XRP\"),\n",
    "    F.col(\"data.prices.cardano\").alias(\"ADA\"),\n",
    "    F.col(\"data.prices.dogecoin\").alias(\"DOGE\"),\n",
    "    F.col(\"data.prices.matic-network\").alias(\"MATIC\"),\n",
    "    F.col(\"data.prices.solana\").alias(\"SOL\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra kết quả để kiểm tra\n",
    "crypto_parsed_df.show(truncate=False)  # Dùng .show() để in ra một vài dòng đầu tiên của DataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from dateutil.relativedelta import relativedelta\n",
    "def get_last_saved_date(crypto_id, storage_path : str):\n",
    "    \"\"\"\n",
    "    Kiểm tra ngày cuối cùng đã được lưu trữ trong GCS hoặc HDFS.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client.from_service_account_json(\"btcanalysishust-b10a2ef12088.json\")\n",
    "    blobs = storage_client.list_blobs(storage_path, prefix=f\"ver2/{crypto_id}/\")\n",
    "    last_saved_date = None\n",
    "    for blob in blobs:\n",
    "        # Trích xuất ngày từ tên thư mục\n",
    "        path_parts = blob.name.split('/')\n",
    "\n",
    "        if len(path_parts) > 2: \n",
    "            year, month = path_parts[2], path_parts[3]\n",
    "            date = datetime.strptime(f\"{year}-{month.zfill(2)}\",\"%Y-%m\")\n",
    "            if last_saved_date is None or date > last_saved_date:\n",
    "                last_saved_date = date\n",
    "\n",
    "    return last_saved_date.strftime(\"%Y-%m\")\n",
    "\n",
    "def get_gcs_price(crypto_id : str, start_date : str , end_date : str):\n",
    "    \"\"\"\n",
    "    Lấy dữ liệu giá từ GCS trong khoảng thời gian chỉ định và hợp nhất thành một bảng Spark.\n",
    "\n",
    "    Args:\n",
    "        crypto_id (str): Tên tài sản (crypto, cổ phiếu, v.v.).\n",
    "        start_date (str): Ngày bắt đầu (định dạng 'YYYY-MM').\n",
    "        end_date (str): Ngày kết thúc (định dạng 'YYYY-MM').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Bảng Spark chứa tất cả dữ liệu giá hợp nhất.\n",
    "    \"\"\"\n",
    "    # Chuyển đổi chuỗi ngày thành đối tượng datetime\n",
    "    start = datetime.strptime(start_date, \"%Y-%m\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m\")\n",
    "    \n",
    "    # Kiểm tra ngày bắt đầu phải nhỏ hơn hoặc bằng ngày kết thúc\n",
    "    if start > end:\n",
    "        raise ValueError(\"start_date phải nhỏ hơn hoặc bằng end_date\")\n",
    "    \n",
    "    # Khởi tạo danh sách kết quả\n",
    "    all_prices = []\n",
    "    \n",
    "    # Lặp qua từng tháng\n",
    "    current = start.replace(day=1)  # Đặt ngày thành ngày đầu tiên của tháng\n",
    "    while current <= end:\n",
    "        curr_price_dir=f\"gs://crypto-historical-data-2/ver2/{crypto_id}/{current.year}/{current.month:02}/data.parquet\"\n",
    "        all_prices.append(spark.read.parquet(curr_price_dir))\n",
    "        if current.month == 12:  # Nếu là tháng 12, chuyển sang tháng 1 năm sau\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    if all_prices:\n",
    "        merged_data = all_prices[0]\n",
    "        for df in all_prices[1:]:\n",
    "            merged_data = merged_data.union(df)\n",
    "        merged_data = merged_data.withColumn('DATE', to_date('DATE', 'yyyy-MM-dd'))\n",
    "        # Sort by the transformed date column\n",
    "        sorted_data = merged_data.orderBy('DATE', ascending= True)\n",
    "\n",
    "        # Sort the DataFrame by the date column\n",
    "        return sorted_data.select(col('DATE'),col('HIGH'),col('LOW'),col('CLOSE'))\n",
    "    else:\n",
    "        raise ValueError(\"Không tìm thấy dữ liệu trong khoảng thời gian được chỉ định.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+\n",
      "|      DATE|    HIGH|     LOW|   CLOSE|\n",
      "+----------+--------+--------+--------+\n",
      "|2024-09-01|59070.55| 57200.0| 57299.0|\n",
      "|2024-09-02| 59423.0|57119.01|59139.83|\n",
      "|2024-09-03| 59825.7|57394.49|57468.84|\n",
      "|2024-09-04|58531.25| 55555.0| 57971.0|\n",
      "|2024-09-05|58326.12|55628.04|56156.82|\n",
      "|2024-09-06| 56995.0| 52530.0|53950.01|\n",
      "|2024-09-07| 54847.0| 53733.1|54156.33|\n",
      "|2024-09-08|55315.95|53623.95|54881.11|\n",
      "|2024-09-09|58119.97|54565.56| 57053.9|\n",
      "|2024-09-10|58050.35|56377.76|57645.59|\n",
      "|2024-09-11|58014.35|55534.41|57352.79|\n",
      "|2024-09-12| 58600.0|57311.15|58137.54|\n",
      "|2024-09-13| 60670.0|57630.01|60543.35|\n",
      "|2024-09-14| 60660.0| 59436.8|60012.35|\n",
      "|2024-09-15|60402.34|58695.75|59122.33|\n",
      "|2024-09-16|59214.15| 57477.0|58208.75|\n",
      "|2024-09-17|61373.41|57620.27| 60312.6|\n",
      "|2024-09-18| 61800.0| 59174.5|61769.18|\n",
      "|2024-09-19|63891.82|61569.16|62960.14|\n",
      "|2024-09-20|64140.67| 62340.0|63210.69|\n",
      "+----------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=get_gcs_price('BTC','2024-09','2024-10')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'%K': 56.3851687986003, '%D': 78.55468205756692}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, max as spark_max, min as spark_min, avg\n",
    "from pyspark.sql.window import Window\n",
    "class STOCK:\n",
    "    def __init__(self, period_k=9, period_d=6, storage_path='crypto-historical-data-2' ):\n",
    "        self.k=period_k\n",
    "        self.d=period_d\n",
    "        self.storage_path=storage_path\n",
    "    def get_data(self,crypto_id):\n",
    "        lastest_month=get_last_saved_date(crypto_id,self.storage_path)\n",
    "        comeback_month=(self.k+self.d-1)//28+1\n",
    "        start_month=datetime.strptime(lastest_month,'%Y-%m')-relativedelta(months=comeback_month)\n",
    "        historical_data_df=get_gcs_price(crypto_id,start_month.strftime('%Y-%m'), lastest_month)\n",
    "        # gia tri gia su\n",
    "        current_close=95285.96\n",
    "        timestap='2024-11-26'\n",
    "        \n",
    "        current_close_df = spark.createDataFrame([(current_close,current_close,current_close, timestap)], [\"CLOSE\",'HIGH',\"LOW\",'DATE'])\n",
    "        combined_data_df = historical_data_df.unionByName(current_close_df,allowMissingColumns=True)\n",
    "        return spark.createDataFrame(combined_data_df.tail(self.k+self.d-1))\n",
    "    def calculate(self, crypto_id):\n",
    "        combined_data_df=self.get_data(crypto_id)\n",
    "        # Kiểm tra cột HIGH và LOW\n",
    "        if not {'HIGH', 'LOW'}.issubset(combined_data_df.columns):\n",
    "            raise ValueError(\"Dữ liệu thiếu cột HIGH hoặc LOW.\")\n",
    "        window_k = Window.orderBy(\"DATE\").rowsBetween(-(self.k - 1), 0)\n",
    "        combined_data_df = combined_data_df.withColumn(f\"HIGH_{self.k}\", spark_max(\"HIGH\").over(window_k))\n",
    "        combined_data_df = combined_data_df.withColumn(f\"LOW_{self.k}\", spark_min(\"LOW\").over(window_k))\n",
    "\n",
    "        # Tính %K\n",
    "        combined_data_df = combined_data_df.withColumn(\n",
    "            \"%K\", \n",
    "            100 * (col(\"CLOSE\") - col(f\"LOW_{self.k}\")) / (col(f\"HIGH_{self.k}\") - col(f\"LOW_{self.k}\"))\n",
    "        ).fillna(0, subset=[\"%K\"])  # Thay NaN bằng 0 nếu (HIGH_k - LOW_k) = 0.\n",
    "        \n",
    "        # Sử dụng cửa sổ để tính trung bình động cho %D\n",
    "        window_d = Window.orderBy(\"DATE\").rowsBetween(-(self.d - 1), 0)\n",
    "        combined_data_df = combined_data_df.withColumn(\"%D\", avg(\"%K\").over(window_d))\n",
    "        \n",
    "        # Trả về DataFrame với %K và %D\n",
    "        return combined_data_df.select( \"%K\", \"%D\").tail(1)[0].asDict()\n",
    "    def calculate__for_all(self,crypto_ids):\n",
    "        all_stocks={}\n",
    "        for crypto_id in crypto_ids:\n",
    "            all_stocks[crypto_id] = self.calculate(crypto_id)\n",
    "        return all_stocks\n",
    "cal_stock=STOCK()\n",
    "df=cal_stock.calculate('BTC')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MACD:\n",
    "    def __init__(self, phien_1, phien_2, storage_path ='crypto-historical-data-2') -> None:\n",
    "        self.phien_1=phien_1\n",
    "        self.phien_2=phien_2\n",
    "        self.storage_path=storage_path\n",
    "    def get_data(self, crypto_id):\n",
    "        ema_dir=f'{crypto_id}'\n",
    "        ema_phien1, ema_phien2=spark.read.parquet(ema_dir)\n",
    "        return ema_phien1,ema_phien2\n",
    "    def calculate(self, crypto_id):\n",
    "        ema_phien1,ema_phien2=self.get_data(crypto_id)\n",
    "        return ema_phien1-ema_phien2\n",
    "    def calculate__for_all(self,crypto_ids):\n",
    "        all_stocks={}\n",
    "        for crypto_id in crypto_ids:\n",
    "            all_stocks[crypto_id] = self.calculate(crypto_id)\n",
    "        return all_stocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
