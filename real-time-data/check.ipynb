{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/ibulmnie/Documents/20241/BigData/crypto-big-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 21:06:34 WARN Utils: Your hostname, MacBook-Pro-cua-My.local resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n",
      "24/12/12 21:06:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/ibulmnie/Documents/20241/BigData/crypto-big-data/spark-env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/ibulmnie/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/ibulmnie/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-06971c58-befd-4521-8474-c8199c37b204;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 911ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-06971c58-befd-4521-8474-c8199c37b204\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/19ms)\n",
      "24/12/12 21:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/12 21:06:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import col, from_json, to_date, avg, lit, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import shutil\n",
    "from pyspark.sql.functions import current_timestamp, col, unix_timestamp\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "gcs_jar_path = os.path.abspath(\"config/gcs-connector-hadoop3-latest.jar\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"hehee\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", \"config/key/btcanalysishust-495a3a227f22.json\") \\\n",
    "    .config(\"spark.jars\", gcs_jar_path) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"btcanalysishust\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"gs://indicator-crypto/sma_results/BTC/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DATE                         |CLOSE   |SMA_5            |SMA_10           |SMA_20           |SMA_50           |SMA_100          |SMA_200          |\n",
      "+-----------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2024-12-12T21:10:14.356+07:00|100968.0|99550.54000000001|99453.99         |97630.995        |86770.63199999997|74535.0154       |68747.13929999998|\n",
      "|2024-12-12T00:00:00.000+07:00|101552.7|99590.94         |98948.94         |97533.79650000001|86083.29419999997|74100.0238       |68584.66789999999|\n",
      "|2024-12-11T00:00:00.000+07:00|101177.0|99260.38         |98379.0          |97378.11600000001|85399.84179999998|73675.8951       |68423.35274999999|\n",
      "|2024-12-10T00:00:00.000+07:00|96655.1 |98999.55999999998|97907.84199999999|97038.9235       |84723.22479999997|73237.1151       |68260.20674999998|\n",
      "|2024-12-09T00:00:00.000+07:00|97399.9 |99086.0          |97991.388        |96827.963        |84170.41999999998|72860.2478       |68116.63349999998|\n",
      "|2024-12-08T00:00:00.000+07:00|101170.0|99357.44000000002|97817.95100000002|96483.983        |83589.69659999997|72477.3765       |67975.18364999998|\n",
      "|2024-12-07T00:00:00.000+07:00|99899.9 |98306.94000000002|97296.07         |95919.8175       |82934.81879999996|72059.3212       |67820.04784999997|\n",
      "|2024-12-06T00:00:00.000+07:00|99872.9 |97497.62         |96499.03400000001|95456.63450000001|82284.86739999997|71650.781        |67677.70919999997|\n",
      "|2024-12-05T00:00:00.000+07:00|97087.3 |96816.12399999998|95811.74500000001|95015.9075       |81639.57819999997|71246.4484       |67509.65279999998|\n",
      "|2024-12-04T00:00:00.000+07:00|98757.1 |96896.776        |95905.83300000001|94528.56250000001|81038.96419999997|70903.9754       |67358.83564999996|\n",
      "|2024-12-03T00:00:00.000+07:00|95917.5 |96278.462        |95808.00000000001|94115.39850000001|80385.13519999998|70558.9237       |67200.28179999997|\n",
      "|2024-12-02T00:00:00.000+07:00|95853.3 |96285.2          |96118.65300000002|93721.295        |79723.69119999999|70241.545        |67046.95519999998|\n",
      "|2024-11-30T00:00:00.000+07:00|96465.42|95500.448        |96377.23200000002|93367.16649999999|79070.37459999998|69923.87920000001|66898.91214999997|\n",
      "|2024-11-29T00:00:00.000+07:00|97490.56|94807.36600000001|96170.005        |92565.34150000001|78391.44119999999|69563.0579       |66724.28424999998|\n",
      "|2024-11-28T00:00:00.000+07:00|95665.53|94914.88999999998|95664.53800000002|91526.6165       |77647.22059999997|69199.78510000002|66551.49329999997|\n",
      "|2024-11-27T00:00:00.000+07:00|95951.19|95337.538        |95150.01499999998|90571.6765       |76945.47559999999|68833.30570000001|66380.43074999997|\n",
      "|2024-11-26T00:00:00.000+07:00|91929.54|95952.106        |94543.56500000002|89570.117        |76268.89219999999|68468.3205       |66204.74794999998|\n",
      "|2024-11-25T00:00:00.000+07:00|93000.01|97254.016        |94414.23500000002|88755.9675       |75674.30699999999|68133.40410000001|66049.03759999998|\n",
      "|2024-11-24T00:00:00.000+07:00|98028.18|97532.644        |94220.06999999999|87575.00349999999|75070.52399999999|67798.37890000001|65899.40539999999|\n",
      "|2024-11-23T00:00:00.000+07:00|97778.77|96414.18600000002|93151.292        |86063.96849999999|74350.98619999998|67406.98700000002|65715.11214999999|\n",
      "+-----------------------------+--------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"BASE\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"OPEN\", FloatType(), True),\n",
    "    StructField(\"HIGH\", FloatType(), True),\n",
    "    StructField(\"LOW\", FloatType(), True),\n",
    "    StructField(\"CLOSE\", FloatType(), True),\n",
    "    StructField(\"VOLUME\", FloatType(), True),\n",
    "    StructField(\"YEAR\", StringType(), True),\n",
    "    StructField(\"MONTH\", StringType(), True),\n",
    "    StructField(\"__index_level_0__\", StringType(), True),\n",
    "    StructField(\"BTC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"ETH_CLOSE\", FloatType(), True),\n",
    "    StructField(\"USDT_CLOSE\", FloatType(), True),\n",
    "    StructField(\"BNB_CLOSE\", FloatType(), True),\n",
    "    StructField(\"USDC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"XRP_CLOSE\", FloatType(), True),\n",
    "    StructField(\"ADA_CLOSE\", FloatType(), True),\n",
    "    StructField(\"MATIC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"DOGE_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SOL_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SMA5_BTC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SMA10_BTC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SMA20_BTC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SMA50_BTC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SMA100_BTC_CLOSE\", FloatType(), True),\n",
    "    StructField(\"SMA200_BTC_CLOSE\", FloatType(), True),\n",
    "])\n",
    "\n",
    "# Read stream with the defined schema\n",
    "df = spark.readStream.schema(schema).parquet(\"gs://indicator-crypto/sma_results/BTC_CLOSE\")\n",
    "\n",
    "# Convert the 'DATE' column to timestamp if it's a string type\n",
    "df = df.withColumn(\"DATE\", F.to_timestamp(\"DATE\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Write the stream to a sink (e.g., console, file, etc.)\n",
    "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
