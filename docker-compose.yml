version: '3.8'

services:
  # PostgreSQL service for storing historical data
  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: huster123
      POSTGRES_DB: crypto_data
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data

  # Kafka broker for real-time data streaming
  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL: PLAINTEXT
      KAFKA_LISTENER_INTERNAL: INSIDE://kafka:9093
      KAFKA_LISTENER_EXTERNAL: EXTERNAL://localhost:9093
      KAFKA_LISTENER_PORT: 9093
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    ports:
      - "9093:9093"
    depends_on:
      - zookeeper

  # Zookeeper service for Kafka
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"

  # Spark Master service for distributed processing
  spark-master:
    image: bitnami/spark:3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"  # Spark UI for monitoring
    volumes:
      - ./spark-data:/bitnami/spark

  # Spark Worker service for distributed processing
  spark-worker:
    image: bitnami/spark:3.3
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
    depends_on:
      - spark-master

  # HDFS service for large-scale storage
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=hadoop-cluster
    ports:
      - "50070:50070"
    volumes:
      - hdfs-namenode-data:/hadoop/dfs/name

  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: hdfs-datanode
    environment:
      - CLUSTER_NAME=hadoop-cluster
    ports:
      - "50075:50075"
    depends_on:
      - hdfs-namenode
    volumes:
      - hdfs-datanode-data:/hadoop/dfs/data

  # Apache Airflow for orchestration of tasks
  airflow:
    image: apache/airflow:latest
    container_name: airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__WORKERS=2
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://admin:huster123@postgres:5432/crypto_data
    ports:
      - "8081:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    depends_on:
      - postgres
      - spark-master
      - kafka

# Python service for handling crypto Kafka producer/consumer
  crypto-kafka:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: crypto-kafka
    volumes:
      - ./real-time-data:/app
    environment:
      - KAFKA_HOST=kafka:9093
    depends_on:
      - kafka
    command: python /app/crypto_kafka_consumer.py  # Thay đổi theo script bạn muốn chạy

  # Python service for collecting and storing historical data
  data-collector:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data-collector
    volumes:
      - ./scripts:/app
    command: python /app/collect_data.py  # Thay đổi theo script bạn muốn chạy

volumes:
  postgres-data:
    driver: local
    driver_opts:
      o: bind
      device: ./data/postgres
  hdfs-namenode-data:
    driver: local
    driver_opts:
      o: bind
      device: ./data/hdfs-namenode
  hdfs-datanode-data:
    driver: local
    driver_opts:
      o: bind
      device: ./data/hdfs-datanode